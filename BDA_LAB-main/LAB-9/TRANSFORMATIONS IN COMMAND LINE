hduser@bmsce-OptiPlex-3060:~$ spark-shell
22/06/28 10:01:31 WARN Utils: Your hostname, bmsce-OptiPlex-3060 resolves to a loopback address: 127.0.1.1; using 10.124.7.72 instead (on interface enp1s0)
22/06/28 10:01:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
22/06/28 10:01:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/06/28 10:01:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Spark context Web UI available at http://10.124.7.72:4041
Spark context available as 'sc' (master = local[*], app id = local-1656390694267).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.8
      /_/
         
Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_312)
Type in expressions to have them evaluated.
Type :help for more information.

scala> val data=sc.textFile("/home/hduser/Desktop/sample.txt");
data: org.apache.spark.rdd.RDD[String] = /home/hduser/Desktop/sample.txt MapPartitionsRDD[1] at textFile at <console>:24

scala> val datafile=sc.textFile("/home/hduser/Desktop/sample.txt");
datafile: org.apache.spark.rdd.RDD[String] = /home/hduser/Desktop/sample.txt MapPartitionsRDD[3] at textFile at <console>:24

scala> val words=datafile.map(x=>x.split(" "))
words: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[4] at map at <console>:25

scala> words.collect()
res0: Array[Array[String]] = Array(Array(hi, hw, are, ypu), Array(how, is, your, job), Array(how, is, your, family), Array(how, is, your, brother), Array(how, is, your, sister))

scala> datafile.map(x=>x.split(" ").length).collect()
res1: Array[Int] = Array(4, 4, 4, 4, 4)

scala> datafile.map(x=>x.split(" ").size).collect()
res2: Array[Int] = Array(4, 4, 4, 4, 4)

scala> datafile.map(x=>x.length).collect()
res3: Array[Int] = Array(13, 15, 18, 19, 18)

scala> datafile.map(x=>x.toUpperCase).collect()
res4: Array[String] = Array(HI HW ARE YPU, HOW IS YOUR JOB, HOW IS YOUR FAMILY, HOW IS YOUR BROTHER, HOW IS YOUR SISTER)

scala> datafile.map(x=>x.toLowerCase).collect()
res5: Array[String] = Array(hi hw are ypu, how is your job, how is your family, how is your brother, how is your sister)

scala> val rdd=sc.parallelize(Seq("how is your job","how is your sister"))
rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[10] at parallelize at <console>:24

scala> rdd.collect
res6: Array[String] = Array(how is your job, how is your sister)

scala> rdd.map(x=>x.split(" ")).collect
res7: Array[Array[String]] = Array(Array(how, is, your, job), Array(how, is, your, sister))

scala> rdd.flatMap(x=>x.split(" ")).collect
res8: Array[String] = Array(how, is, your, job, how, is, your, sister)

scala> rdd.map(x=>x.split(" ")).count()
res9: Long = 2

scala> rdd.flatMap(x=>x.split(" ")).count()
res10: Long = 8

scala> datafile.flatMap(x=>x.split(" ")).map(x=>(x,x.length)).collect
res11: Array[(String, Int)] = Array((hi,2), (hw,2), (are,3), (ypu,3), (how,3), (is,2), (your,4), (job,3), (how,3), (is,2), (your,4), (family,6), (how,3), (is,2), (your,4), (brother,7), (how,3), (is,2), (your,4), (sister,6))

scala> rdd.collect
res12: Array[String] = Array(how is your job, how is your sister)

scala> rdd.filter(x=>x.contains("your")).collect
res13: Array[String] = Array(how is your job, how is your sister)

scala> rdd.filter(x=>x.contains("sister")).collect
res14: Array[String] = Array(how is your sister)

scala> sc.parallelize(1 to 4).filter(x=>(x%2==0)).collect
res15: Array[Int] = Array(2, 4)

