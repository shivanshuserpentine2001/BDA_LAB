package wordcount12;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;

import org.apache.hadoop.mapred.Reporter;
public class WCMapper extends MapReduceBase implements
Mapper<LongWritable,Text, Text, IntWritable>{
// Map function
public void map(LongWritable key, Text value, OutputCollector<Text,
IntWritable> output, Reporter rep) throws IOException

{ int i=0;
ArrayList<Integer> arr = new ArrayList<Integer>();
//int arr[]=new int[6];
String line = value.toString();


for (String word : line.split(" "))
{
if (word.length() > 0)
{
arr.add(Integer.parseInt((word)));
//output.collect(new Text("maxnumber"), new IntWritable(Integer.parseInt((word))));
}
}
Collections.sort(arr);
for(i=0;i<arr.size();i++) {
output.collect(new Text("maxi"), new IntWritable(arr.get(i)));
//}
}
}

}


reducer

package wordcount12;
import java.io.IOException;
import java.util.Iterator;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter;

public class WCReducer extends MapReduceBase implements
Reducer<Text,IntWritable, Text, IntWritable>{
// Reduce function


@Override
public void reduce(Text key, Iterator<IntWritable> value, OutputCollector<Text, IntWritable> output, Reporter arg3)
throws IOException {
// TODO Auto-generated method stub
while(value.hasNext()) {
IntWritable i = value.next();

output.collect(key,(i));
}
}
}